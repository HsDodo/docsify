### MySql 八股

**可以按四个角度来分类索引:**

- 按 **数据结构** 分类：`B+ tree 索引` 、`Hash索引`、`Full-text索引`
- 按 **物理存储** 分类：`聚簇索引（主键索引）`、`二级索引（辅助索引）`
- 按 **字段特性** 分类：`主键索引`、`唯一索引`、`普通索引`、`前缀索引`
- 按 **字段个数** 分类：`单列索引`、`联合索引`



#### Compact 行格式长什么样？

![image-20240626175110721](images/image-20240626175110721.png) 

#### 为什么 MySQL InnoDB 选择 B+tree 作为索引的数据结构？

***1、B+Tree vs B Tree***

对于有 N 个叶子节点的 B+Tree，其搜索复杂度为`O(logdN)`，其中 d 表示节点允许的最大子节点个数为 d 个。

在实际的应用当中， d 值是大于100的，这样就保证了，即使数据达到千万级别时，B+Tree 的高度依然维持在 3 ~ 4 层左右，也就是说一次数据查询操作只需要做 3 ~ 4 次的磁盘 I/O 操作就能查询到目标数据。

而二叉树的每个父节点的儿子节点个数只能是 2 个，意味着其搜索复杂度为 `O(logN)`，这已经比 B+Tree 高出不少，因此二叉树检索到目标数据所经历的磁盘 I/O 次数要更多。

***2、B+Tree vs 二叉树***

对于有 N 个叶子节点的 B+Tree，其搜索复杂度为`O(logdN)`，其中 d 表示节点允许的最大子节点个数为 d 个。

在实际的应用当中， d 值是大于100的，这样就保证了，即使数据达到千万级别时，B+Tree 的高度依然维持在 3~4 层左右，也就是说一次数据查询操作只需要做 3~4 次的磁盘 I/O 操作就能查询到目标数据。

而二叉树的每个父节点的儿子节点个数只能是 2 个，意味着其搜索复杂度为 `O(logN)`，这已经比 B+Tree 高出不少，因此二叉树检索到目标数据所经历的磁盘 I/O 次数要更多。

***3、B+Tree vs Hash***

Hash 在做等值查询的时候效率贼快，搜索复杂度为 O(1)。

但是 Hash 表不适合做范围查询，它更适合做等值的查询，这也是 B+Tree 索引要比 Hash 表索引有着更广泛的适用场景的原因。

#### 联合索引什么时候才能被有效利用？

> **关键** ： 利用索引的前提是索引中的 key 是有序的 

使用联合索引时，存在 **最左匹配原则**, 也就是按照最左优先的方式进行索引的匹配。

#### 索引失效的10种场景 

参考 

1. **不满足最左匹配原则**
2. **索引列上有计算**

```sql
explain select * from 某表 where id+1=2;
```

3. **索引列用了函数**

```sql
explain select * from 某表 where SUBSTR(height,1,2)=17;
```

4. **字段类型不同**

> 因为 MySQL 在遇到字符串和数字进行比较的时候, 会自动把字符串转为数字, 然后进行比较

比如

```sql
select * from user_table where phone = 1300000001
```

因为phone字段为字符串, 所以MySQL 要自动把字符串转为数字, 所以这条语句相当于:

```sql
select * from t_user where CAST(phone AS signed int) = 1300000001
```

CAST 函数作用在了 phone 字段，而phone字段是索引, 也就是对索引使用了函数, 而对索引使用函数是会导致索引失效的



5. **使用了 `select *`**

```sql
explain 
select * from 某表 where name='苏三';

```

使用`SELECT *`本身并不会直接导致联合索引的“失效”。这里的“失效”通常指的是索引没有被查询优化器选择来加速查询过程。`SELECT *`表示查询表中的所有列，它与索引是否被使用无关，而是关乎数据检索的范围和方式。

联合索引是否被有效利用，主要取决于查询的WHERE子句和ORDER BY、GROUP BY等子句中对索引列的使用情况，以及是否遵循最左前缀原则等，如前所述。只要查询条件能够匹配联合索引的定义，索引就有可能被使用，不论查询语句中是使用`SELECT *`还是显式指定列名。

然而，值得注意的是，虽然`SELECT *`不影响索引的使用，但它可能会影响查询的性能，尤其是在表中有大量列或者大文本、二进制等大数据类型列的情况下。这是因为`SELECT *`会加载表中的所有列，即使某些列的数据并未实际使用。因此，从性能优化的角度出发，推荐在查询时仅选择需要的列，而不是盲目使用`SELECT *`。

6. **like 左边包含 %** 

> **目前like查询主要有三种情况：** 1、`like '%a'`  2、`like 'a%'`  3、`like '%a%'`

```sql
explain select * from 某表
where code like '%1%';
```

7. **列对比**

```sql
explain select * from user 
where id=height
```

8. **使用 or 关键字**

> 如果使用了`or`关键字，那么它前面和后面的字段都要加索引，不然所有的索引都会失效，这是一个大坑

如：id、height 是索引，address不是

```sql
explain select * from user 
where id=1 or height='175'; # n能正常走索引
```

下面的不能走索引

```sql
explain select * from user 
where id=1 or height='175' or address='成都'; 
```

9. **not in 和 not exists**

主键字段中使用not in关键字查询数据范围，任然可以走索引。而普通索引字段使用了not in关键字查询数据范围，索引会失效 

**除此之外，如果sql语句中使用`not exists`时，索引也会失效。具体sql语句如下：**

```sql
explain select * from user  t1
where  not exists (select 1 from user t2 where t2.height=173 and t1.id=t2.id)
```

**`order by`后面如果包含了联合索引的多个排序字段，只要它们的排序规律是相同的（要么同时升序，要么同时降序），也可以走索引。**

如果`order by`语句中没有加where或limit关键字，该sql语句将不会走索引

#### MySQL 是怎么加行级锁的？

参考🚀[MySQL 是怎么加锁的？ | 小林coding (xiaolincoding.com)](https://xiaolincoding.com/mysql/lock/how_to_lock.html#唯一索引等值查询)

> 首先要明白：普通的 select 语句是不加锁的，因为是快照读。如果使用当前读的话，会进行加锁处理 (`Next-Key Lock`)， 但是 `Next-Key Lock` 在某些情况是会退化成 `记录锁` 或者 `间隙锁`。

##### 唯一索引等值查询

1. 在记录「**存在**」的情况下，因为是唯一索引，是不能再插入索引相同的记录（会报键冲突错误），所以就用不上**间隙锁**，这时候就只需要 **记录锁** 来保证这条记录不被删除！

2. 唯一索引等值查询并且查询记录「**不存在**」的场景下，在索引树找到第一条大于该查询记录的记录后，要将该记录的索引中的 `next-key lock` 会退化成「`间隙锁`」, 因为要锁住间隙，使得其他事务不能往这个间隙里面添加记录，不用 **记录锁** 的原因是，这条记录不存在，也就不用加记录锁来保证这条记录不被删除。

##### 唯一索引范围查询

当唯一索引进行范围查询时，**会对每一个扫描到的索引加 next-key 锁，然后如果遇到下面这些情况，会退化成记录锁或者间隙锁**：

- 情况一：针对「大于等于」的范围查询，因为存在等值查询的条件，那么如果等值查询的记录是存在于表中，那么该记录的索引中的 next-key 锁会**退化成记录锁**。
- 情况二：针对「小于或者小于等于」的范围查询，要看条件值的记录是否存在于表中：
  - 当条件值的记录不在表中，那么不管是「小于」还是「小于等于」条件的范围查询，**扫描到终止范围查询的记录时，该记录的索引的 next-key 锁会退化成间隙锁**，其他扫描到的记录，都是在这些记录的索引上加 next-key 锁。
  - 当条件值的记录在表中，如果是「小于」条件的范围查询，**扫描到终止范围查询的记录时，该记录的索引的 next-key 锁会退化成间隙锁**，其他扫描到的记录，都是在这些记录的索引上加 next-key 锁；如果「小于等于」条件的范围查询，扫描到终止范围查询的记录时，该记录的索引 next-key 锁不会退化成间隙锁。其他扫描到的记录，都是在这些记录的索引上加 next-key 锁。

#### 什么是 Redo Log ？

![image-20240629145307668](images/image-20240629145307668.png)

redo log 是物理日志，记录了某个数据页做了什么修改，比如**对 XXX 表空间中的 YYY 数据页 ZZZ 偏移量的地方做了AAA 更新**，每当执行一个事务就会产生这样的一条或者多条物理日志。

在事务提交时，只要先将 redo log 持久化到磁盘即可，可以不需要等到将缓存在 Buffer Pool 里的脏页数据持久化到磁盘。

当系统崩溃时，虽然脏页数据没有持久化，但是 redo log 已经持久化，接着 MySQL 重启后，可以根据 redo log 的内容，将所有数据恢复到最新的状态。

##### 被修改 Undo 页面，需要记录对应 redo log 吗？

需要的。

开启事务后，InnoDB 层更新记录前，首先要记录相应的 undo log，如果是更新操作，需要把被更新的列的旧值记下来，也就是要生成一条 undo log，undo log 会写入 Buffer Pool 中的 Undo 页面。

不过，**在内存修改该 Undo 页面后，需要记录对应的 redo log**。

#### Undo Log 和 Redo Log 有什么区别 ?

这两种日志是属于 InnoDB 存储引擎的日志，它们的区别在于：

- **redo log** 记录了此次事务「**完成后**」的数据状态，记录的是更新**之后**的值；
- **undo log** 记录了此次事务「**开始前**」的数据状态，记录的是更新**之前**的值；

事务提交之前发生了崩溃，重启后会通过 `undo log` 回滚事务，事务提交之后发生了崩溃，重启后会通过 `redo log` 恢复事务，如下图：

![image-20240629145413733](images/image-20240629145413733.png) 

所以有了 redo log，再通过 WAL 技术，InnoDB 就可以保证即使数据库发生异常重启，之前已提交的记录都不会丢失，这个能力称为 **crash-safe**（崩溃恢复）。可以看出来， **redo log 保证了事务四大特性中的持久性**。

#### 什么是 WAL 技术 ？ 

Buffer Pool 是提高了读写效率没错，但是问题来了，Buffer Pool 是基于内存的，而内存总是不可靠，万一断电重启，还没来得及落盘的脏页数据就会丢失。

为了防止断电导致数据丢失的问题，当有一条记录需要更新的时候，InnoDB 引擎就会先更新内存（同时标记为脏页），然后将本次对这个页的修改以 redo log 的形式记录下来，**这个时候更新就算完成了**。

后续，InnoDB 引擎会在适当的时候，由后台线程将缓存在 Buffer Pool 的脏页刷新到磁盘里，这就是 **WAL （Write-Ahead Logging）技术**。

**WAL 技术指的是， MySQL 的写操作并不是立刻写到磁盘上，而是先写日志，然后在合适的时间再写到磁盘上**。

#### 说说 MySQL 中的 ACID 的具体实现 ？

在MySQL中，特别是使用InnoDB存储引擎时，ACID（原子性、一致性、隔离性、持久性）属性是通过多种机制和技术来实现的。下面是这些特性的具体实现细节：

1. **原子性(Atomicity)**

   - 实现方式：通过`undo logs`和`redo logs`。`undo logs`用于回滚事务，`redo logs`用于崩溃恢复，确保事务要么完全执行，要么完全不执行。

2. **一致性(Consistency)**

   - 实现方式：主要依靠`MVCC（多版本并发控制）`。MVCC允许读取操作看到过去的数据版本，同时不影响正在进行的写操作，保持数据的一致性。

3. **隔离性(Isolation)**

   - 实现方式：通过不同级别的锁定机制和`MVCC`。InnoDB支持四种隔离级别，每个级别决定了事务之间如何交互，以及读取数据的可见性。

   | **隔离级别**   | 效果                                                         |
   | -------------- | ------------------------------------------------------------ |
   | 读未提交（RU） | 一个事务还没提交时，它做的变更就能被别的事务看到。（别的事务指同一时间进行的增删改查操作） |
   | 读提交（RC）   | 一个事务提交（commit）之后，它做的变更才会被其他事务看到。每次查询都会生成一个新的 `read view` |
   | 可重复读（RR） | 一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的。 当然在可重复读隔离级别下，未提交变更对其他事务也是不可见的。在事务开始时创建一次 `read view` |
   | 串行化（S）    | 正如物理书上写的，串行是单线路，顾名思义在MySQL中同一时刻只允许单个事务执行，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行。 |

4. **持久性(Durability)**

   - 实现方式：主要依赖于`redo logs`和`检查点(checkpoint)`。`redo logs`确保事务提交后数据的持久性，`检查点`定期将内存中的数据同步到磁盘，减少恢复时间。

简单来说，原子性通过`undo`和`redo`日志保证，一致性由`MVCC`支持，隔离性通过锁定和`MVCC`的不同组合实现，而持久性则由`redo logs`和定期的`检查点`机制来保障。

#### redo log 要写到磁盘，数据也要写磁盘，为什么要多此一举？

写入 redo log 的方式使用了追加操作， 所以磁盘操作是**顺序写**，而写入数据需要先找到写入位置，然后才写到磁盘，所以磁盘操作是**随机写**。

磁盘的「顺序写 」比「随机写」 高效的多，因此 redo log 写入磁盘的开销更小。

针对「顺序写」为什么比「随机写」更快这个问题，可以比喻为你有一个本子，按照顺序一页一页写肯定比写一个字都要找到对应页写快得多。

可以说这是 WAL 技术的另外一个优点：**MySQL 的写操作从磁盘的「随机写」变成了「顺序写」**，提升语句的执行性能。这是因为 MySQL 的写操作并不是立刻更新到磁盘上，而是先记录在日志上，然后在合适的时间再更新到磁盘上 。

至此， 针对为什么需要 redo log 这个问题我们有两个答案：

- **实现事务的持久性，让 MySQL 有 crash-safe 的能力**，能够保证 MySQL 在任何时间段突然崩溃，重启后之前已提交的记录都不会丢失；
- **将写操作从「随机写」变成了「顺序写」**，提升 MySQL 写入磁盘的性能。

#### 产生的 redo log 是直接写入磁盘的吗？

不是的。

实际上， 执行一个事务的过程中，产生的 redo log 也不是直接写入磁盘的，因为这样会产生大量的 I/O 操作，而且磁盘的运行速度远慢于内存。

所以，redo log 也有自己的缓存—— **redo log buffer**，每当产生一条 redo log 时，会先写入到 redo log buffer，后续在持久化到磁盘。

#### redo log 什么时候刷盘？

缓存在 redo log buffer 里的 redo log 还是在内存中，它什么时候刷新到磁盘？

主要有下面几个时机：

- MySQL 正常关闭时；
- 当 redo log buffer 中记录的写入量大于 redo log buffer 内存空间的一半时，会触发落盘；
- InnoDB 的后台线程每隔 1 秒，将 redo log buffer 持久化到磁盘。
- 每次事务提交时都将缓存在 redo log buffer 里的 redo log 直接持久化到磁盘（这个策略可由 innodb_flush_log_at_trx_commit 参数控制，下面会说）。

#### innodb_flush_log_at_trx_commit 参数控制的是什么？

单独执行一个更新语句的时候，InnoDB 引擎会自己启动一个事务，在执行更新语句的过程中，生成的 redo log 先写入到 redo log buffer 中，然后等事务提交的时候，再将缓存在 redo log buffer 中的 redo log 按组的方式「顺序写」到磁盘。

上面这种 redo log 刷盘时机是在事务提交的时候，这个默认的行为。

除此之外，InnoDB 还提供了另外两种策略，由参数 `innodb_flush_log_at_trx_commit` 参数控制，可取的值有：0、1、2，默认值为 1，这三个值分别代表的策略如下：

- 当设置该**参数为 0 时**，表示每次事务提交时 ，还是**将 redo log 留在 redo log buffer 中** ，该模式下在事务提交时不会主动触发写入磁盘的操作。
- 当设置该**参数为 1 时**，表示每次事务提交时，都**将缓存在 redo log buffer 里的 redo log 直接持久化到磁盘**，这样可以保证 MySQL 异常重启之后数据不会丢失。
- 当设置该**参数为 2 时**，表示每次事务提交时，都只是缓存在 redo log buffer 里的 redo log **写到 redo log 文件，注意写入到「 redo log 文件」并不意味着写入到了磁盘**，因为操作系统的文件系统中有个 Page Cache（如果你想了解 Page Cache，可以看[这篇 (opens new window)](https://xiaolincoding.com/os/6_file_system/pagecache.html)），Page Cache 是专门用来缓存文件数据的，所以写入「 redo log文件」意味着写入到了操作系统的文件缓存。

![image-20240629152417379](images/image-20240629152417379.png)

加入了后台现线程后，innodb_flush_log_at_trx_commit 的刷盘时机如下图：

![image-20240629152616290](images/image-20240629152616290.png)

#### Redo Log 和 BinLog 的区别 ？

这两个日志有四个区别。

*1、**适用对象不同：***

- binlog 是 MySQL 的 Server 层实现的日志，所有存储引擎都可以使用；
- redo log 是 Innodb 存储引擎实现的日志；

*2、**文件格式不同：***

- binlog 有 3 种格式类型，分别是 STATEMENT（默认格式）、ROW、 MIXED，区别如下：
  - STATEMENT：每一条修改数据的 SQL 都会被记录到 binlog 中（相当于记录了逻辑操作，所以针对这种格式， binlog 可以称为逻辑日志），主从复制中 slave 端再根据 SQL 语句重现。但 STATEMENT 有动态函数的问题，比如你用了 uuid 或者 now 这些函数，你在主库上执行的结果并不是你在从库执行的结果，这种随时在变的函数会导致复制的数据不一致；
  - ROW：记录行数据最终被修改成什么样了（这种格式的日志，就不能称为逻辑日志了），不会出现 STATEMENT 下动态函数的问题。但 ROW 的缺点是每行数据的变化结果都会被记录，比如执行批量 update 语句，更新多少行数据就会产生多少条记录，使 binlog 文件过大，而在 STATEMENT 格式下只会记录一个 update 语句而已；
  - MIXED：包含了 STATEMENT 和 ROW 模式，它会根据不同的情况自动使用 ROW 模式和 STATEMENT 模式；
- redo log 是物理日志，记录的是在某个数据页做了什么修改，比如对 XXX 表空间中的 YYY 数据页 ZZZ 偏移量的地方做了AAA 更新；

*3、**写入方式不同**：*

- binlog 是追加写，写满一个文件，就创建一个新的文件继续写，不会覆盖以前的日志，保存的是全量的日志。
- redo log 是循环写，日志空间大小是固定，全部写满就从头开始，保存未被刷入磁盘的脏页日志。

*4、**用途不同：***

- binlog 用于备份恢复、主从复制；
- redo log 用于掉电等故障恢复。

#### 主从复制是怎么实现？

MySQL 的主从复制依赖于 binlog ，也就是记录 MySQL 上的所有变化并以二进制形式保存在磁盘上。复制的过程就是将 binlog 中的数据从主库传输到从库上。

这个过程一般是**异步**的，也就是主库上执行事务操作的线程不会等待复制 binlog 的线程同步完成。

![image-20240629160557740](images/image-20240629160557740.png)

MySQL 集群的主从复制过程梳理成 3 个阶段：

- **写入 Binlog**：主库写 binlog 日志，提交事务，并更新本地存储数据。
- **同步 Binlog**：把 binlog 复制到所有从库上，每个从库把 binlog 写到暂存日志中。
- **回放 Binlog**：回放 binlog，并更新存储引擎中的数据。

具体详细过程如下：

- MySQL 主库在收到客户端提交事务的请求之后，会先写入 binlog，再提交事务，更新存储引擎中的数据，事务提交完成后，返回给客户端“操作成功”的响应。
- 从库会创建一个专门的 I/O 线程，连接主库的 log dump 线程，来接收主库的 binlog 日志，再把 binlog 信息写入 relay log 的中继日志里，再返回给主库“复制成功”的响应。
- 从库会创建一个用于回放 binlog 的线程，去读 relay log 中继日志，然后回放 binlog 更新存储引擎中的数据，最终实现主从的数据一致性。

在完成主从复制之后，你就可以在写数据时只写主库，在读数据时只读从库，这样即使写请求会锁表或者锁记录，也不会影响读请求的执行。

#### update 语句的执行过程

当优化器分析出成本最小的执行计划后，执行器就按照执行计划开始进行更新操作。

具体更新一条记录 `UPDATE t_user SET name = 'xiaolin' WHERE id = 1;` 的流程如下:

1. 执行器负责具体执行，会调用存储引擎的接口，通过主键索引树搜索获取 id = 1 这一行记录：
   - 如果 id=1 这一行所在的数据页本来就在 buffer pool 中，就直接返回给执行器更新；
   - 如果记录不在 buffer pool，将数据页从磁盘读入到 buffer pool，返回记录给执行器。
2. 执行器得到聚簇索引记录后，会看一下更新前的记录和更新后的记录是否一样：
   - 如果一样的话就不进行后续更新流程；
   - 如果不一样的话就把更新前的记录和更新后的记录都当作参数传给 InnoDB 层，让 InnoDB 真正的执行更新记录的操作；
3. 开启事务， InnoDB 层更新记录前，首先要记录相应的 undo log，因为这是更新操作，需要把被更新的列的旧值记下来，也就是要生成一条 undo log，undo log 会写入 Buffer Pool 中的 Undo 页面，不过在内存修改该 Undo 页面后，需要记录对应的 redo log。
4. InnoDB 层开始更新记录，会先更新内存（同时标记为脏页），然后将记录写到 redo log 里面，这个时候更新就算完成了。为了减少磁盘I/O，不会立即将脏页写入磁盘，后续由后台线程选择一个合适的时机将脏页写入到磁盘。这就是 **WAL 技术**，MySQL 的写操作并不是立刻写到磁盘上，而是先写 redo 日志，然后在合适的时间再将修改的行数据写到磁盘上。
5. 至此，一条记录更新完了。
6. 在一条更新语句执行完成后，然后开始记录该语句对应的 binlog，此时记录的 binlog 会被保存到 binlog cache，并没有刷新到硬盘上的 binlog 文件，在事务提交时才会统一将该事务运行过程中的所有 binlog 刷新到硬盘。
7. 事务提交，剩下的就是「两阶段提交」的事情了，接下来就讲这个。

#### 为什么需要两阶段提交？

事务提交后，redo log 和 binlog 都要持久化到磁盘，但是这两个是独立的逻辑，可能出现半成功的状态，这样就造成两份日志之间的逻辑不一致。

举个例子，假设 id = 1 这行数据的字段 name 的值原本是 'jay'，然后执行 `UPDATE t_user SET name = 'xiaolin' WHERE id = 1;` 如果在持久化 redo log 和 binlog 两个日志的过程中，出现了半成功状态，那么就有两种情况：

- **如果在将 redo log 刷入到磁盘之后， MySQL 突然宕机了，而 binlog 还没有来得及写入**。MySQL 重启后，通过 redo log 能将 Buffer Pool 中 id = 1 这行数据的 name 字段恢复到新值 xiaolin，但是 binlog 里面没有记录这条更新语句，在主从架构中，binlog 会被复制到从库，由于 binlog 丢失了这条更新语句，从库的这一行 name 字段是旧值 jay，与主库的值不一致性；
- **如果在将 binlog 刷入到磁盘之后， MySQL 突然宕机了，而 redo log 还没有来得及写入**。由于 redo log 还没写，崩溃恢复以后这个事务无效，所以 id = 1 这行数据的 name 字段还是旧值 jay，而 binlog 里面记录了这条更新语句，在主从架构中，binlog 会被复制到从库，从库执行了这条更新语句，那么这一行 name 字段是新值 xiaolin，与主库的值不一致性；

可以看到，在持久化 redo log 和 binlog 这两份日志的时候，如果出现半成功的状态，就会造成主从环境的数据不一致性。这是因为 redo log 影响主库的数据，binlog 影响从库的数据，所以 redo log 和 binlog 必须保持一致才能保证主从数据一致。

**MySQL 为了避免出现两份日志之间的逻辑不一致的问题，使用了「两阶段提交」来解决**，两阶段提交其实是分布式事务一致性协议，它可以保证多个逻辑操作要不全部成功，要不全部失败，不会出现半成功的状态。

**两阶段提交把单个事务的提交拆分成了 2 个阶段，分别是「准备（Prepare）阶段」和「提交（Commit）阶段」**，每个阶段都由协调者（Coordinator）和参与者（Participant）共同完成。注意，不要把提交（Commit）阶段和 commit 语句混淆了，commit 语句执行的时候，会包含提交（Commit）阶段。

举个拳击比赛的例子，两位拳击手（参与者）开始比赛之前，裁判（协调者）会在中间确认两位拳击手的状态，类似于问你准备好了吗？

- <font color='#d2b42c'>准备阶段</font>：裁判（协调者）会依次询问两位拳击手（参与者）是否准备好了，然后拳击手听到后做出应答，如果觉得自己准备好了，就会跟裁判说准备好了；如果没有自己还没有准备好（比如拳套还没有带好），就会跟裁判说还没准备好。
- <font color='#d2b42c'>提交阶段</font>：如果两位拳击手（参与者）都回答准备好了，裁判（协调者）宣布比赛正式开始，两位拳击手就可以直接开打；如果任何一位拳击手（参与者）回答没有准备好，裁判（协调者）会宣布比赛暂停，对应事务中的回滚操作。

#### MySQL 磁盘 I/O 很高，有什么优化方法？

现在我们知道事务在提交的时候，需要将 binlog 和 redo log 持久化到磁盘，那么如果出现 MySQL 磁盘 I/O 很高的现象，我们可以通过控制以下参数，来 “延迟” binlog 和 redo log 刷盘的时机，从而降低磁盘 I/O 的频率：

- 设置组提交的两个参数： binlog_group_commit_sync_delay 和 binlog_group_commit_sync_no_delay_count 参数，延迟 binlog 刷盘的时机，从而减少 binlog 的刷盘次数。这个方法是基于“额外的故意等待”来实现的，因此可能会增加语句的响应时间，但即使 MySQL 进程中途挂了，也没有丢失数据的风险，因为 binlog 早被写入到 page cache 了，只要系统没有宕机，缓存在 page cache 里的 binlog 就会被持久化到磁盘。
- 将 sync_binlog 设置为大于 1 的值（比较常见是 100~1000），表示每次提交事务都 write，但累积 N 个事务后才 fsync，相当于延迟了 binlog 刷盘的时机。但是这样做的风险是，主机掉电时会丢 N 个事务的 binlog 日志。
- 将 innodb_flush_log_at_trx_commit 设置为 2。表示每次事务提交时，都只是缓存在 redo log buffer 里的 redo log 写到 redo log 文件，注意写入到「 redo log 文件」并不意味着写入到了磁盘，因为操作系统的文件系统中有个 Page Cache，专门用来缓存文件数据的，所以写入「 redo log文件」意味着写入到了操作系统的文件缓存，然后交由操作系统控制持久化到磁盘的时机。但是这样做的风险是，主机掉电的时候会丢数据。

#### 为什么要有 Buffer Pool ？

虽然说 MySQL 的数据是存储在磁盘里的，但是也不能每次都从磁盘里面读取数据，这样性能是极差的。

要想提升查询性能，加个缓存就行了嘛。所以，当数据从磁盘中取出后，缓存内存中，下次查询同样的数据的时候，直接从内存中读取。

为此，Innodb 存储引擎设计了一个**缓冲池（Buffer Pool）**，来提高数据库的读写性能。

##### 什么是预读失败？

先来说说 MySQL 的预读机制。程序是有空间局部性的，靠近当前被访问数据的数据，在未来很大概率会被访问到。

所以，MySQL 在加载数据页时，会提前把它相邻的数据页一并加载进来，目的是为了减少磁盘 IO。

但是可能这些<font color='#d2b42c'>被提前加载进来的数据页，并没有被访问</font>，相当于这个预读是白做了，这个就是<font color='#d2b42c'>预读失效</font>。

如果使用简单的 LRU 算法，就会把预读页放到 LRU 链表头部，而当 Buffer Pool空间不够的时候，还需要把末尾的页淘汰掉。

如果这些预读页如果一直不会被访问到，就会出现一个很奇怪的问题，不会被访问的预读页却占用了 LRU 链表前排的位置，而末尾淘汰的页，可能是频繁访问的页，这样就大大降低了缓存命中率。

##### 怎么解决预读失效而导致缓存命中率降低的问题？

我们不能因为害怕预读失效，而将预读机制去掉，大部分情况下，局部性原理还是成立的。

要避免预读失效带来影响，最好就是**让预读的页停留在 Buffer Pool 里的时间要尽可能的短，让真正被访问的页才移动到 LRU 链表的头部，从而保证真正被读取的热数据留在 Buffer Pool 里的时间尽可能长**。

那到底怎么才能避免呢？

MySQL 是这样做的，它改进了 LRU 算法，将 LRU 划分了 2 个区域：**old 区域 和 young 区域**。

young 区域在 LRU 链表的前半部分，old 区域则是在后半部分，如下图：

![image-20240630103739889](images/image-20240630103739889.png)

old 区域占整个 LRU 链表长度的比例可以通过 `innodb_old_blocks_pct` 参数来设置，默认是 37，代表整个 LRU 链表中 young 区域与 old 区域比例是 63:37。

**划分这两个区域后，预读的页就只需要加入到 old 区域的头部，当页被真正访问的时候，才将页插入 young 区域的头部**。如果预读的页一直没有被访问，就会从 old 区域移除，这样就不会影响 young 区域中的热点数据。

虽然通过划分 old 区域 和 young 区域避免了预读失效带来的影响，但是还有个问题无法解决，那就是 Buffer Pool 污染的问题。

##### 什么是 Buffer Pool 污染问题 ？

当某一个 SQL 语句<font color='#d2b42c'>扫描了大量的数据</font>时，在 Buffer Pool 空间比较有限的情况下，可能会将 <font color='#d2b42c'>Buffer Pool 里的所有页都替换出去，导致大量热数据被淘汰了</font>，等这些热数据又被再次访问的时候，由于缓存未命中，就会产生大量的磁盘 IO，MySQL 性能就会急剧下降，这个过程被称为**<font color='#d2b42c'>Buffer Pool 污染</font>**。

注意， Buffer Pool 污染并不只是查询语句查询出了大量的数据才出现的问题，即使查询出来的结果集很小，也会造成 Buffer Pool 污染。

比如，在一个数据量非常大的表，执行了这条语句：

```sql
select * from t_user where name like "%xiaolin%";
```

可能这个查询出来的结果就几条记录，但是由于这条语句会发生索引失效，所以这个查询过程是全表扫描的，接着会发生如下的过程：

- 从磁盘读到的页加入到 LRU 链表的 old 区域头部；
- 当从页里读取行记录时，也就是页被访问的时候，就要将该页放到 young 区域头部；
- 接下来拿行记录的 name 字段和字符串 xiaolin 进行模糊匹配，如果符合条件，就加入到结果集里；
- 如此往复，直到扫描完表中的所有记录。

经过这一番折腾，原本 young 区域的热点数据都会被替换掉。

##### 怎么解决出现 Buffer Pool 污染而导致缓存命中率下降的问题？

像前面这种全表扫描的查询，很多缓冲页其实只会被访问一次，但是它却只因为被访问了一次而进入到 young 区域，从而导致热点数据被替换了。

LRU 链表中 young 区域就是热点数据，只要我们提高进入到 young 区域的门槛，就能有效地保证 young 区域里的热点数据不会被替换掉。

MySQL 是这样做的，进入到 young 区域条件增加了一个**停留在 old 区域的时间判断**。

具体是这样做的，在对某个处在 old 区域的缓存页进行第一次访问时，就在它对应的控制块中记录下来这个访问时间：

- 如果后续的访问时间与第一次访问的时间**在某个时间间隔内**，那么**该缓存页就不会被从 old 区域移动到 young 区域的头部**；
- 如果后续的访问时间与第一次访问的时间**不在某个时间间隔内**，那么**该缓存页移动到 young 区域的头部**；

这个间隔时间是由 `innodb_old_blocks_time` 控制的，默认是 1000 ms。

也就说，**只有同时满足「被访问」与「在 old 区域停留时间超过 1 秒」两个条件，才会被插入到 young 区域头部**，这样就解决了 Buffer Pool 污染的问题 。

另外，MySQL 针对 young 区域其实做了一个优化，为了防止 young 区域节点频繁移动到头部。young 区域前面 1/4 被访问不会移动到链表头部，只有后面的 3/4被访问了才会。

#### 脏页什么时候会刷入磁盘 ？

引入了 Buffer Pool 后，当修改数据时，首先是修改 Buffer Pool 中数据所在的页，然后将其页设置为脏页，但是磁盘中还是原数据。

因此，脏页需要被刷入磁盘，保证缓存和磁盘数据一致，但是若每次修改数据都刷入磁盘，则性能会很差，因此一般都会在一定时机进行批量刷盘。

可能大家担心，如果在脏页还没有来得及刷入到磁盘时，MySQL 宕机了，不就丢失数据了吗？

这个不用担心，InnoDB 的更新操作采用的是 Write Ahead Log 策略，即先写日志，再写入磁盘，通过 redo log 日志让 MySQL 拥有了崩溃恢复能力。

下面几种情况会触发脏页的刷新：

- 当 redo log 日志满了的情况下，会主动触发脏页刷新到磁盘；
- Buffer Pool 空间不足时，需要将一部分数据页淘汰掉，如果淘汰的是脏页，需要先将脏页同步到磁盘；
- MySQL 认为空闲时，后台线程会定期将适量的脏页刷入到磁盘；
- MySQL 正常关闭之前，会把所有的脏页刷入到磁盘；

在我们开启了慢 SQL 监控后，如果你发现**「偶尔」会出现一些用时稍长的 SQL**，这可能是因为脏页在刷新到磁盘时可能会给数据库带来性能开销，导致数据库操作抖动。

如果间断出现这种现象，就需要调大 Buffer Pool 空间或 redo log 日志的大小。

#### 缓存池（Buffer Pool）总结 

Innodb 存储引擎设计了一个**缓冲池（Buffer Pool）**，来提高数据库的读写性能。

Buffer Pool 以页为单位缓冲数据，可以通过 `innodb_buffer_pool_size` 参数调整缓冲池的大小，默认是 128 M。

Innodb 通过三种链表来管理缓页：

- Free List （空闲页链表），管理空闲页；
- Flush List （脏页链表），管理脏页；
- LRU List，管理脏页+干净页，将最近且经常查询的数据缓存在其中，而不常查询的数据就淘汰出去。；

InnoDB 对 LRU 做了一些优化，我们熟悉的 LRU 算法通常是将最近查询的数据放到 LRU 链表的头部，而 InnoDB 做 2 点优化：

- 将 LRU 链表 分为**young 和 old 两个区域**，加入缓冲池的页，优先插入 old 区域；页被访问时，才进入 young 区域，目的是为了解决预读失效的问题。
- 当**「页被访问」且「 old 区域停留时间超过 `innodb_old_blocks_time` 阈值（默认为1秒）」**时，才会将页插入到 young 区域，否则还是插入到 old 区域，目的是为了解决批量数据访问，大量热数据淘汰的问题。

可以通过调整 `innodb_old_blocks_pct` 参数，设置 young 区域和 old 区域比例。

在开启了慢 SQL 监控后，如果你发现「偶尔」会出现一些用时稍长的 SQL，这可因为脏页在刷新到磁盘时导致数据库性能抖动。如果在很短的时间出现这种现象，就需要调大 Buffer Pool 空间或 redo log 日志的大小。

#### MySQL中char和varchar用作主键的话，哪一个好，为什么?

在MySQL中选择`CHAR`或`VARCHAR`作为表的主键时，主要考虑的因素包括性能、存储效率以及数据的一致性。

**CHAR**

`CHAR`类型会分配固定长度的空间来存储字符数据。如果插入的数据长度小于定义的长度，MySQL会在字符串的末尾填充空格来达到所需的长度。当用于主键时，`CHAR`类型的优势在于：

- **固定长度**：对于所有记录来说，`CHAR`类型的主键占用相同的字节数，这使得索引维护更加简单，查询性能可能会更好。
- **无须额外处理**：因为长度固定，所以不需要额外的指针或者偏移量来确定字符串的实际长度，这样在读取时可能会更快一些。

**VARCHAR**

`VARCHAR`类型则分配可变长度的空间来存储字符数据。这意味着实际存储的字节数会随着插入的数据长度变化。如果使用`VARCHAR`作为主键，其优点包括：

- **节省空间**：因为`VARCHAR`只存储实际数据的长度，所以相比于`CHAR`，它更节省存储空间，特别是在存储较短字符串时。
- **灵活性**：适合于不知道或者无法固定长度的场景。

**性能与存储考虑**

从性能角度来看，在大多数情况下，如果主键字符串长度固定，使用`CHAR`可能会提供更好的性能，因为定长的字符串更容易优化索引结构。然而，如果字符串长度变化较大，使用`VARCHAR`可以节省大量的存储空间，并且对于现代MySQL版本来说，性能差距已经不是那么明显了。

从存储角度来看，`VARCHAR`显然更为经济，尤其是在存储大量较短字符串的情况下。

**综合建议**

- 如果你的主键长度固定，并且长度不是特别长（例如，UUID），使用`CHAR`可以简化索引管理，提升性能。
- 如果你的主键长度变化较大，或者出于节省存储空间的考虑，使用`VARCHAR`更为合适。

#### 大表如何添加索引？

> 如果一张表的数据量是千万级别的，那么，如何给这张表添加索引呢？
>
> 需要知道的一点是，**给表添加索引的时候，是会对表进行加锁的**，如果不慎操作，有可能会产生事故，可以参考以下方法：

1. 先创建一张跟原表`A`数据结构相同的`B`表
2. 在新表`B` 添加上索引
3. 把原表`A`的数据导入到`B`表中
4. `rename`新表 `B` 为原表的表名`A` ，原表更换为其他表名

#### SQL中的DML、DDL以及DCL是什么？

`DML`（data manipulation language）是数据操纵语言：它们是SELECT、UPDATE、INSERT、DELETE，就象它的名字一样，这4条命令是用来对数据库里的数据进行操作的语言。

`DDL`（data definition language）是数据定义语言：DDL比DML要多，主要的命令有CREATE、ALTER、DROP等，DDL主要是用在定义或改变表（TABLE）的结构，数据类型，表之间的链接和约束等初始化工作上，他们大多在建立表时使用。

`DCL`（DataControlLanguage）是数据库控制语言：是用来设置或更改数据库用户或角色权限的语句，包括（grant,deny,revoke等）语句。

#### drop、delete 与 truncate 的区别？

三者都表示删除，但是三者有一些差别：

| 区别     | delete                                   | truncate                       | drop                                               |
| -------- | ---------------------------------------- | ------------------------------ | -------------------------------------------------- |
| 类型     | 属于 `DML`                               | 属于 `DDL`                     | 属于 `DDL`                                         |
| 回滚     | 可回滚                                   | 不可回滚                       | 不可回滚                                           |
| 删除内容 | 表结构还在，删除表的全部或者一部分数据行 | 表结构还在，删除表中的所有数据 | 从数据库中删除表，所有数据行，索引和权限也会被删除 |
| 删除速度 | 删除速度慢，需要逐行删除                 | 删除速度快                     | 删除速度最快                                       |

因此，在不再需要一张表的时候，用 drop；在想删除部分数据行时候，用 delete；在保留表而删除所有数据的时候用 truncate。

#### UNION 和 UNION ALL 的区别？

- 如果使用 UNION，会在表链接后筛选掉重复的记录行
- 如果使用 UNION ALL，不会合并重复的记录行
- 从效率上说，UNION ALL 要比 UNION 快很多，如果合并没有刻意要删除重复行，那么就使用 UNION All

#### count(1)、count(*) 与 count(列名) 的区别？

**执行效果**：

- count(*)包括了所有的列，相当于行数，在统计结果的时候，不会忽略列值为 NULL
- count(1)包括了忽略所有列，用 1 代表代码行，在统计结果的时候，不会忽略列值为 NULL
- count(列名)只包括列名那一列，在统计结果的时候，会忽略列值为空（这里的空不是只空字符串或者 0，而是表示 null）的计数，即某个字段值为 NULL 时，不统计。

**执行速度**：

- 列名为主键，count(列名)会比 count(1)快
- 列名不为主键，count(1)会比 count(列名)快
- 如果表多个列并且没有主键，则 count（1） 的执行效率优于 count（*）
- 如果有主键，则 select count（主键）的执行效率是最优的
- 如果表只有一个字段，则 select count（*）最优。

#### 一条 SQL 查询语句的执行顺序？

SQL查询语句的执行顺序与编写顺序并不完全一致。理解这一点对于优化查询性能和正确构建复杂的SQL语句非常重要。以下是SQL查询语句的标准执行顺序：

1. **FROM** 子句：首先处理 `FROM` 子句，确定要从哪些表中获取数据。如果涉及到多个表，会根据 `JOIN` 条件将这些表连接起来形成一个临时的结果集。
2. **ON** 子句：在 `FROM` 和 `JOIN` 之后立即执行 `ON` 子句，用来指定如何关联两个或多个表。只有满足 `ON` 条件的数据行才会被保留下来。
3. **OUTER JOIN 的额外处理**：对于 `LEFT JOIN`, `RIGHT JOIN` 或 `FULL OUTER JOIN`，此时会添加那些没有匹配到的记录，并用NULL填充相应的列。
4. **WHERE** 子句：接着是 `WHERE` 子句的执行，它用于过滤掉不符合条件的数据行。只有通过了 `WHERE` 条件筛选后的行才会进入下一步。
5. **GROUP BY** 子句：对剩余的数据进行分组。如果有 `GROUP BY` 子句，那么结果集中的每一组都会产生一行输出。
6. **HAVING** 子句：这一步是对分组后的数据应用过滤条件。`HAVING` 通常用于聚合函数（如 `SUM()`, `COUNT()` 等），并且只作用于已经分组的数据。
7. **SELECT** 子句：然后是 `SELECT` 子句的执行，选择需要展示给用户的列。这里可以使用表达式、别名等来计算或重命名列。
8. **DISTINCT** 关键字：如果 `SELECT` 中包含了 `DISTINCT` 关键字，则在这一步去除重复的行。
9. **ORDER BY** 子句：最后执行 `ORDER BY` 子句，对最终的结果集按照指定的列进行排序。
10. **LIMIT/OFFSET** 子句：在某些数据库系统（如MySQL）中，还可以使用 `LIMIT` 和 `OFFSET` 来限制返回结果的数量及起始位置。

总结来说，尽管我们在写SQL时通常先写 `SELECT` 再写 `FROM`，但实际执行过程中却是从 `FROM` 开始，经过一系列步骤后才到达 `SELECT`。了解这个过程有助于更好地设计和调试SQL查询。

#### 介绍一下 MySQL bin 目录下的可执行文件？

- mysql：客户端程序，用于连接 MySQL 服务器
- mysqldump：一个非常实用的 MySQL 数据库备份工具，用于创建一个或多个 MySQL 数据库级别的 SQL 转储文件，包括数据库的表结构和数据。对数据备份、迁移或恢复非常重要。
- mysqladmin：mysql 后面加上 admin 就表明这是一个 MySQL 的管理工具，它可以用来执行一些管理操作，比如说创建数据库、删除数据库、查看 MySQL 服务器的状态等。
- mysqlcheck：mysqlcheck 是 MySQL 提供的一个命令行工具，用于检查、修复、分析和优化数据库表，对数据库的维护和性能优化非常有用。
- mysqlimport：用于从文本文件中导入数据到数据库表中，非常适合用于批量导入数据。
- mysqlshow：用于显示 MySQL 数据库服务器中的数据库、表、列等信息。
- mysqlbinlog：用于查看 MySQL 二进制日志文件的内容，可以用于恢复数据、查看数据变更等。

#### MySQL 第 3-10 条记录怎么查？

在 MySQL 中，要查询第 3 到第 10 条记录，可以使用 limit 语句，结合偏移量 offset 和行数 row_count 来实现。

limit 语句用于限制查询结果的数量，偏移量表示从哪条记录开始，行数表示返回的记录数量。



```sql
SELECT * FROM table_name LIMIT 2, 8;
```

- 2：偏移量，表示跳过前两条记录，从第三条记录开始。
- 8：行数，表示从偏移量开始，返回 8 条记录。

偏移量是从 0 开始的，即第一条记录的偏移量是 0；如果想从第 3 条记录开始，偏移量就应该是 2。

#### 用过哪些 MySQL 函数？

MySQL 支持很多内置函数，包括执行计算、格式转换、日期处理等。我说一些自己常用的（挑一些自己熟悉的）。

#### [用过哪些字符串函数来处理文本？](https://javabetter.cn/sidebar/sanfene/mysql.html#用过哪些字符串函数来处理文本)

- `CONCAT()`: 连接两个或多个字符串。
- `LENGTH()`: 返回字符串的长度。
- `SUBSTRING()`: 从字符串中提取子字符串。
- `REPLACE()`: 替换字符串中的某部分。
- `LOWER()` 和 `UPPER()`: 分别将字符串转换为小写或大写。
- `TRIM()`: 去除字符串两侧的空格或其他指定字符。

##### 用过哪些数值函数？

- `ABS()`: 返回一个数的绝对值。
- `CEILING()`: 返回大于或等于给定数值的最小整数。
- `FLOOR()`: 返回小于或等于给定数值的最大整数。
- `ROUND()`: 四舍五入到指定的小数位数。
- `MOD()`: 返回除法操作的余数。

##### 用过哪些日期和时间函数？

- `NOW()`: 返回当前的日期和时间。
- `CURDATE()`: 返回当前的日期。
- `CURTIME()`: 返回当前的时间。
- `DATE_ADD()` 和 `DATE_SUB()`: 在日期上加上或减去指定的时间间隔。
- `DATEDIFF()`: 返回两个日期之间的天数。
- `DAY()`, `MONTH()`, `YEAR()`: 分别返回日期的日、月、年部分。

##### 用过哪些汇总函数？

- `SUM()`: 计算数值列的总和。
- `AVG()`: 计算数值列的平均值。
- `COUNT()`: 计算某列的行数。
- `MAX()` 和 `MIN()`: 分别返回列中的最大值和最小值。
- `GROUP_CONCAT()`: 将多个行值连接为一个字符串。

##### 用过哪些逻辑函数？

- `IF()`: 如果条件为真，则返回一个值；否则返回另一个值。
- `CASE`: 根据一系列条件返回值。
- `COALESCE()`: 返回参数列表中的第一个非 NULL 值。

#### 说说mysql的两阶段提交？

##### 两阶段提交过程

1. **准备阶段（Prepare Phase）**:
   - 事务的所有更改首先被写入 InnoDB 的重做日志。
   - 重做日志被刷新到磁盘以确保持久化。
   - 此时，事务处于“准备就绪”状态，但还没有实际提交到数据文件中。
2. **提交阶段（Commit Phase）**:
   - MySQL 写入二进制日志条目，并将该条目刷新到磁盘。
   - 只有当二进制日志写入成功后，MySQL 才会告诉 InnoDB 可以提交事务。
   - InnoDB 将事务标记为已提交，并释放相关的锁。
   - 事务的更改最终被应用到数据文件中，但这个步骤可能稍后发生，因为 InnoDB 可能会延迟实际的数据页写入。

##### 保证一致性

通过这样的顺序，MySQL 确保了以下几点：

- **持久性**：由于重做日志在准备阶段就已经写入磁盘，因此即使在提交阶段之前服务器崩溃，也可以通过重做日志恢复事务。
- **一致性**：如果在写入二进制日志的过程中发生崩溃，那么在重启后，MySQL 可以回滚那些已经准备但未提交的事务，从而保证 binlog 和 InnoDB 数据的一致性。
- **复制安全性**：由于 binlog 记录了完整的事务变更，所以可以安全地用于主从复制，而不会出现部分事务的情况。

这种机制确保了即使在极端情况下（如服务器崩溃），数据库也能恢复到一个一致的状态，并且主从复制不会受到不完整事务的影响。

#### 说说分库分表的场景和作用？

说实话垂直方向（即业务方向）更简单。

在水平方向（即数据方向）上，分库和分表的作用，其实是有区别的，不能混为一谈。

- 分库：是为了解决数据库连接资源不足问题，和磁盘IO的性能瓶颈问题。

- 分表：是为了解决单表数据量太大，sql语句查询数据时，即使走了索引也非常耗时问题。此外还可以解决消耗cpu资源问题。
- 分库分表：可以解决 数据库连接资源不足、磁盘IO的性能瓶颈、检索数据耗时 和 消耗cpu资源等问题。

如果在有些业务场景中，用户并发量很大，但是需要保存的数据量很少，这时可以只分库，不分表。

如果在有些业务场景中，用户并发量不大，但是需要保存的数量很多，这时可以只分表，不分库。

如果在有些业务场景中，用户并发量大，并且需要保存的数量也很多时，可以分库分表。

